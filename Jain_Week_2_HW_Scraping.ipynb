{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130896c8-109d-49f1-bce9-30fc0bcbccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f153367e-b63b-4d9e-8d7b-b7d382dec500",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import randint necessary library\n",
    "from random import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8d9d24-e76c-4479-a4b4-efe5adb961cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import logging\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd987bc3-d2c6-45d1-964f-eedb802477a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e0dcd4c-ead7-4fe1-96cb-e68b495661ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## headers\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"           \n",
    "                         \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                         \"Chrome/124.0.0.0 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef61e72-58b8-4338-aa4a-7cd6dc1e5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that for the below cell ChatGPT told me to install certain python libraries - probably didn't need all of them - to ensure that the pd.read_html() function can work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d07555c-8556-48c3-906f-c9be7c889968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.12/site-packages (5.2.1)\n",
      "Requirement already satisfied: html5lib in /opt/anaconda3/lib/python3.12/site-packages (1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: six>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.12/site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml html5lib beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f53b186-cf19-4133-b78e-6d3b60a07c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snoozing for 5.951958196122211 seconds before next scrape\n",
      "done scraping all tables\n"
     ]
    }
   ],
   "source": [
    "## full scrape example of multiple pages with error handling and snoozing\n",
    "url = \"https://www.baseball-reference.com/leagues/majors/2020-free-agents.shtml\"\n",
    "df_list = []\n",
    "broken_links = []\n",
    "\n",
    "## read all tables into a list of dataframes (used ChatGPT for help with this)\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "## got this from chat GPT when I asked it how to loop tables first with a snooze. \n",
    "for i, df in enumerate(tables, start=1):\n",
    "  \n",
    "    try:\n",
    "    ## df = pd.read_html(url)[0] I don't understand what this function does from the class demo, but don't think we need since we are not dealing with urls with different pages\n",
    "        df[\"source_url\"] = url\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Oh no...encountered an inssue:{e} at {url}\")\n",
    "        broken_table.append(url)\n",
    "    finally:\n",
    "        snoozer = uniform(3,9)\n",
    "        print(f\"Snoozing for {snoozer} seconds before next scrape\")\n",
    "        time.sleep(snoozer)\n",
    "    \n",
    "    print(\"done scraping all tables\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca4f034-82ff-4a50-95a2-8c3ce5bef702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped table saved into a CSV\n"
     ]
    }
   ],
   "source": [
    "## Now I need to define the df so I can save the tables to a csv\n",
    "## Indicated 0 because I only need the first table on the the webpage\n",
    "\n",
    "df=tables[0]\n",
    "\n",
    "# saving it to my comp\n",
    "df.to_csv(\"Baseball_Scrape.csv\", index=False)\n",
    "print(\"Scraped table saved into a CSV\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
